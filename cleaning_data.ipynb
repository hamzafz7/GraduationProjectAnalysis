{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: youtube-transcript-api in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (0.6.3)\n",
      "Requirement already satisfied: defusedxml<0.8.0,>=0.7.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from youtube-transcript-api) (0.7.1)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from youtube-transcript-api) (2.32.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->youtube-transcript-api) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->youtube-transcript-api) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->youtube-transcript-api) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->youtube-transcript-api) (2.0.4)\n"
     ]
    }
   ],
   "source": [
    "! pip install youtube-transcript-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        video_title     video_id    channel_name  \\\n",
      "0  انتهاكات ومجازر في الساحل السوري  Jw0qhd3JHtc  Ahmad Fakhouri   \n",
      "1  انتهاكات ومجازر في الساحل السوري  Jw0qhd3JHtc  Ahmad Fakhouri   \n",
      "2  انتهاكات ومجازر في الساحل السوري  Jw0qhd3JHtc  Ahmad Fakhouri   \n",
      "3  انتهاكات ومجازر في الساحل السوري  Jw0qhd3JHtc  Ahmad Fakhouri   \n",
      "4  انتهاكات ومجازر في الساحل السوري  Jw0qhd3JHtc  Ahmad Fakhouri   \n",
      "\n",
      "                                          video_tags  \\\n",
      "0  أحمد الشرع, بشار الأسد, أخبار سوريا, الانتهاكا...   \n",
      "1  أحمد الشرع, بشار الأسد, أخبار سوريا, الانتهاكا...   \n",
      "2  أحمد الشرع, بشار الأسد, أخبار سوريا, الانتهاكا...   \n",
      "3  أحمد الشرع, بشار الأسد, أخبار سوريا, الانتهاكا...   \n",
      "4  أحمد الشرع, بشار الأسد, أخبار سوريا, الانتهاكا...   \n",
      "\n",
      "                                             comment  likes_on_comment  \\\n",
      "0  انا كنت أحد متابعي قناة الجزيرة و كنت اعتبرها ...               906   \n",
      "1  كلامك يحمل بين طياته تبرير للجرائم و المجازر و...               345   \n",
      "2  لعن الله كل من طمس الحقيقة واخفاها \\nرحم الله ...               359   \n",
      "3  له يا أستاذ احمد..\\nلا تبرر الانتهاكات... أغلب...               143   \n",
      "4  بسم الله الرحمن الرحيم\\nولا تحسبن الله غافلا ع...               462   \n",
      "\n",
      "   reply_count  comment_length  relevant  question  suggestion  offensive  \\\n",
      "0          121             162         0         0           0          0   \n",
      "1           14              87         1         0           0          1   \n",
      "2           17             101         1         0           0          1   \n",
      "3            7             112         1         0           0          1   \n",
      "4           21             118         0         0           0          1   \n",
      "\n",
      "  transcript  \n",
      "0       None  \n",
      "1       None  \n",
      "2       None  \n",
      "3       None  \n",
      "4       None  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "with open('C:\\\\Users\\\\User\\\\Desktop\\\\hamza2.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "df = pd.DataFrame(data) \n",
    "df['transcript'] = None\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "\n",
    "transcript_cache = {}\n",
    "\n",
    "def get_transcript_cached(video_id):\n",
    "    if video_id in transcript_cache:\n",
    "        return transcript_cache[video_id] \n",
    "\n",
    "    try:\n",
    "        transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=['ar'])\n",
    "        text = \" \".join([segment['text'] for segment in transcript])\n",
    "        transcript_cache[video_id] = text  # Cache it\n",
    "        return text\n",
    "    except Exception:\n",
    "        transcript_cache[video_id] = None\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in df.iterrows():\n",
    "    if pd.isnull(row['transcript']):\n",
    "        video_id = row['video_id']\n",
    "        df.at[idx, 'transcript'] = get_transcript_cached(video_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B_Mx4JRmGro' 'BsKX8OM0Cq4' 'yeMbo4KI8RM' 'TuO2ON3Nla0' 'a0v9cB6R9xY'\n",
      " 'qpQwbE4TpX8' 'B1aKhni0Lzg' 'LXerUX1Awmo' 'YNPCqiq1Ubk' 'AV05KzyRgiY'\n",
      " 'gPCAqEwG8us' 'ON7V2XiQFrM' '0Ro2djOERr4']\n"
     ]
    }
   ],
   "source": [
    "missing_transcripts = df[df['transcript'].isnull()]\n",
    "\n",
    "video_ids_missing = missing_transcripts['video_id'].unique()\n",
    "\n",
    "print(video_ids_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "594"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['transcript'].isna().sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['transcript']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install frasa stanza arabic-reshaper python-bidi pandas pyarabic\n",
    "! python -m stanza.download -d ar  # Download Arabic models for Stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: stanza in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (1.10.1)\n",
      "Collecting farasapy\n",
      "  Downloading farasapy-0.0.14-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: pandas in c:\\programdata\\anaconda3\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (1.26.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from stanza) (4.67.1)\n",
      "Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\lib\\site-packages (from stanza) (2.7.1)\n",
      "Requirement already satisfied: emoji in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from stanza) (2.14.1)\n",
      "Requirement already satisfied: torch>=1.3.0 in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from stanza) (2.6.0)\n",
      "Requirement already satisfied: protobuf>=3.15.0 in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from stanza) (4.25.4)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from stanza) (2.32.3)\n",
      "Requirement already satisfied: tomli in c:\\programdata\\anaconda3\\lib\\site-packages (from stanza) (1.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from torch>=1.3.0->stanza) (3.6.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from torch>=1.3.0->stanza) (2024.6.1)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch>=1.3.0->stanza) (2.11.3)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from torch>=1.3.0->stanza) (1.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from torch>=1.3.0->stanza) (4.12.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch>=1.3.0->stanza) (1.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.3.0->stanza) (2.0.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->stanza) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->stanza) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->stanza) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->stanza) (2.0.4)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm->stanza) (0.4.4)\n",
      "Installing collected packages: farasapy\n",
      "Successfully installed farasapy-0.0.14\n"
     ]
    }
   ],
   "source": [
    "! pip install stanza farasapy pandas numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from farasa.segmenter import FarasaSegmenter\n",
    "from farasa.stemmer import FarasaStemmer\n",
    "from farasa.ner import FarasaNamedEntityRecognizer\n",
    "\n",
    "# Initialize Farasa tools (requires API key)\n",
    "segmenter = FarasaSegmenter(interactive=True)  # for word segmentation\n",
    "stemmer = FarasaStemmer(interactive=True)     # for stemming\n",
    "ner = FarasaNamedEntityRecognizer(interactive=True)  # for NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "\n",
    "stanza.download('ar')\n",
    "nlp = stanza.Pipeline('ar', processors='tokenize,lemma,pos,ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from farasa.segmenter import FarasaSegmenter\n",
    "\n",
    "def clean_arabic_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Remove diacritics (harakat)\n",
    "    text = re.sub(r'[\\u064B-\\u065F\\u0670]', '', text)\n",
    "    \n",
    "    # Normalize Alef variations (أ, إ, آ → ا)\n",
    "    text = re.sub(r'[أإآ]', 'ا', text)\n",
    "    \n",
    "    # Remove extra whitespace and non-Arabic characters\n",
    "    text = re.sub(r'[^\\u0600-\\u06FF\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply cleaning to the transcript column\n",
    "df['cleaned_transcript'] = df['transcript'].apply(clean_arabic_text) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "346     عام ميلادي في الجزيره العربيه وفي خضم نار الحر...\n",
       "347     عام ميلادي في الجزيره العربيه وفي خضم نار الحر...\n",
       "348     عام ميلادي في الجزيره العربيه وفي خضم نار الحر...\n",
       "349     عام ميلادي في الجزيره العربيه وفي خضم نار الحر...\n",
       "350     عام ميلادي في الجزيره العربيه وفي خضم نار الحر...\n",
       "                              ...                        \n",
       "3513    خامس اكبر اسباب الضعف الجنسي خامس احسن حلول تس...\n",
       "3514    خامس اكبر اسباب الضعف الجنسي خامس احسن حلول تس...\n",
       "3516    خامس اكبر اسباب الضعف الجنسي خامس احسن حلول تس...\n",
       "3517    خامس اكبر اسباب الضعف الجنسي خامس احسن حلول تس...\n",
       "3518    خامس اكبر اسباب الضعف الجنسي خامس احسن حلول تس...\n",
       "Name: cleaned_transcript, Length: 1311, dtype: object"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['cleaned_transcript']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# Download Arabic model if not already installed\n",
    "stanza.download('ar')\n",
    "\n",
    "# Initialize Stanza pipeline for POS tagging and NER\n",
    "nlp = stanza.Pipeline('ar', processors='tokenize,pos,ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import re\n",
    "# Initialize cache\n",
    "transcript_cache = {}  # {video_id: keywords_entities_list}\n",
    "\n",
    "def process_transcript(video_id, transcript):\n",
    "    \"\"\"Process transcript with proper None handling\"\"\"\n",
    "    if video_id in transcript_cache:\n",
    "        return transcript_cache[video_id]\n",
    "    \n",
    "    try:\n",
    "        doc = nlp(transcript)\n",
    "        \n",
    "        # Extract keywords (handle None lemmas)\n",
    "        keywords = []\n",
    "        for sentence in doc.sentences:\n",
    "            for word in sentence.words:\n",
    "                if word.upos in ['NOUN', 'ADJ']:\n",
    "                    lemma = word.lemma if word.lemma is not None else word.text\n",
    "                    keywords.append(lemma.lower())\n",
    "        \n",
    "        # Get top keywords (now with safe processing)\n",
    "        top_keywords = [word for word, _ in Counter(keywords).most_common(50)]\n",
    "        \n",
    "        # Extract entities (with duplicate removal)\n",
    "        entities = list({\n",
    "            (ent.text.lower() if ent.text else \"\", ent.type)\n",
    "            for ent in doc.ents\n",
    "            if ent.type in ['PERSON', 'LOC', 'ORG'] and ent.text\n",
    "        })\n",
    "        \n",
    "        # Combine and deduplicate\n",
    "        combined = [e[0] for e in entities if e[0]] + top_keywords\n",
    "        seen = set()\n",
    "        keywords_entities = [x for x in combined if x and not (x in seen or seen.add(x))]\n",
    "        \n",
    "        transcript_cache[video_id] = keywords_entities\n",
    "        return keywords_entities\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing video {video_id}: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "# Process data\n",
    "video_data = df.drop_duplicates('video_id')[['video_id', 'cleaned_transcript']]\n",
    "for _, row in video_data.iterrows():\n",
    "    process_transcript(row['video_id'], row['cleaned_transcript'])\n",
    "\n",
    "# Add to dataframe\n",
    "df['keywords_entities'] = df['video_id'].map(transcript_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['comment'] = df['comment'].apply(clean_arabic_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['comment_length'] = df['comment'].str.replace(r'\\s', '', regex=True).str.len()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_count = len(df)\n",
    "df = df[df['comment_length'] >= 4]  # Keep only comments with ≥4 letters\n",
    "removed_count = initial_count - len(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
